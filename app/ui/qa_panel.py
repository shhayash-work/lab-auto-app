"""
RAGè³ªç–‘å¿œç­”ãƒ‘ãƒãƒ«UI
æ¤œè¨¼ãƒãƒƒãƒã®å®Ÿè¡Œçµæœã‚’ãƒ™ã‚¯ã‚¿ãƒ¼DBã«ä¿å­˜ã—ã¦ã€è³ªå•ã«å¿œã˜ã¦RAGã§é–¢é€£ãƒãƒƒãƒã‚’èª­ã¿å‡ºã—â†’AIãŒå›ç­”ã‚’ç”Ÿæˆâ†’ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§è¡¨ç¤º
"""
import streamlit as st
import logging
from typing import List, Dict, Any, Optional, Callable
import json
from datetime import datetime, timedelta

from app.services.llm_service import get_llm_service
from app.services.vector_store import get_vector_store
from app.services.batch_storage import load_realistic_batches

logger = logging.getLogger(__name__)

def render_qa_panel():
    """RAGè³ªç–‘å¿œç­”ãƒ‘ãƒãƒ«ã‚’è¡¨ç¤º"""
    st.markdown("## AIè³ªç–‘å¿œç­”")
    st.markdown("æ¤œè¨¼ãƒãƒƒãƒã®å®Ÿè¡Œçµæœã«é–¢ã™ã‚‹è³ªå•ã«AIãŒå›ç­”ã—ã¾ã™ï¼ˆRAGæŠ€è¡“ä½¿ç”¨ï¼‰")
    
    # è¡¨ç¤ºè¨­å®š
    st.markdown("**è¡¨ç¤ºè¨­å®š**")
    col_s1, col_s2, col_spacer = st.columns([2, 2, 6])
    with col_s1:
        use_streaming = st.checkbox("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º", value=True, help="å›ç­”ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤º")
    with col_s2:
        show_thinking = st.checkbox("æ€è€ƒéç¨‹è¡¨ç¤º", value=False, help="AIã®æ€è€ƒéç¨‹ã‚’è¡¨ç¤º")
    
    st.divider()
    
    # LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰å–å¾—ï¼‰
    selected_provider = st.session_state.get('selected_provider', 'ollama')
    
    # è³ªå•å¿œç­”ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    render_qa_interface(selected_provider, use_streaming, show_thinking)

def render_qa_interface(llm_provider: str, use_streaming: bool = True, show_thinking: bool = False):
    """è³ªå•å¿œç­”ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’è¡¨ç¤º"""
    st.markdown("### æ¤œè¨¼çµæœã«ã¤ã„ã¦è³ªå•ã™ã‚‹")
    st.markdown("éå»ã®æ¤œè¨¼ãƒãƒƒãƒã®çµæœã‚„çµ±è¨ˆæƒ…å ±ã«ã¤ã„ã¦è³ªå•ã§ãã¾ã™")
    
    # ã‚µãƒ³ãƒ—ãƒ«è³ªå•
    st.write("**ã‚µãƒ³ãƒ—ãƒ«è³ªå•:**")
    sample_questions = [
        "æœ€è¿‘å¤±æ•—ã—ãŸæ¤œè¨¼é …ç›®ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "åŸºåœ°å±€ã‚¹ãƒªãƒ¼ãƒ—æ©Ÿèƒ½ã®æ¤œè¨¼çµæœã‚’æ•™ãˆã¦ãã ã•ã„",
        "Ericsson-MMUè¨­å‚™ã§å•é¡ŒãŒç™ºç”Ÿã—ãŸæ¤œè¨¼ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "æˆåŠŸç‡ãŒä½ã„æ¤œè¨¼é …ç›®ã‚’ç‰¹å®šã—ã¦ãã ã•ã„",
        "ä»Šæ—¥å®Ÿè¡Œã•ã‚ŒãŸæ¤œè¨¼ã®çµæœã¯ã©ã†ã§ã—ãŸã‹ï¼Ÿ",
        "ç‰¹å®šã®è¨­å‚™ã‚¿ã‚¤ãƒ—ã§é »ç¹ã«å¤±æ•—ã™ã‚‹æ¤œè¨¼é …ç›®ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
    ]
    
    selected_question = st.selectbox(
        "ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’é¸æŠï¼ˆã¾ãŸã¯ä¸‹ã«ç‹¬è‡ªã®è³ªå•ã‚’å…¥åŠ›ï¼‰",
        ["è³ªå•ã‚’é¸æŠ..."] + sample_questions
    )
    
    # è³ªå•å…¥åŠ›
    if selected_question != "è³ªå•ã‚’é¸æŠ...":
        question = st.text_input("è³ªå•å†…å®¹:", value=selected_question)
    else:
        question = st.text_input("è³ªå•å†…å®¹:")
    
    # AIè³ªå•ãƒœã‚¿ãƒ³
    ask_button = st.button("AIã«è³ªå•ã™ã‚‹", type="primary", use_container_width=True)
    
    if ask_button:
        if question:
            try:
                # LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
                llm_service = get_llm_service(llm_provider)
                
                # RAGã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œå¯è¦–åŒ–
                with st.spinner("ğŸ” é–¢é€£æ¤œè¨¼ãƒãƒƒãƒã‚’æ¤œç´¢ä¸­..."):
                    # ãƒªã‚¢ãƒ«ãªãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    realistic_batches = load_realistic_batches()
                    
                    # ãƒ™ã‚¯ã‚¿ãƒ¼æ¤œç´¢ã®å®Ÿè¡Œï¼ˆãƒ†ã‚¹ãƒˆé …ç›®ã‹ã‚‰ï¼‰
                    vector_store = get_vector_store()
                    test_item_results = vector_store.search(question, top_k=3)
                    
                    # ç›´æ¥ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é–¢é€£ã™ã‚‹ã‚‚ã®ã‚’æ¤œç´¢
                    batch_results = _search_batches_directly(question, realistic_batches, top_k=5)
                    
                    # çµæœã‚’çµ±åˆ
                    search_results = batch_results
                    
                    if search_results:
                        st.success(f"âœ… {len(search_results)}ä»¶ã®é–¢é€£ãƒãƒƒãƒã‚’ç™ºè¦‹")
                        
                        # æ¤œç´¢çµæœã®è¡¨ç¤ºï¼ˆè©³ç´°è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if show_thinking:
                            with st.expander("ğŸ” æ¤œç´¢ã•ã‚ŒãŸé–¢é€£ãƒãƒƒãƒ", expanded=False):
                                for i, result in enumerate(search_results):
                                    st.write(f"**{i+1}. {result.get('name', 'Unknown')}**")
                                    st.write(f"- å®Ÿè¡Œæ—¥: {result.get('created_at', 'Unknown')}")
                                    st.write(f"- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.get('status', 'Unknown')}")
                                    st.write(f"- è©¦é¨“ãƒ–ãƒ­ãƒƒã‚¯: {result.get('test_block', 'Unknown')}")
                                    if result.get('results'):
                                        success_count = len([r for r in result['results'] if r.get('result') == 'PASS'])
                                        total_count = len(result['results'])
                                        st.write(f"- æˆåŠŸç‡: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
                    else:
                        st.warning("é–¢é€£ã™ã‚‹æ¤œè¨¼ãƒãƒƒãƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                
                # æ€è€ƒéç¨‹è¡¨ç¤º
                if show_thinking:
                    with st.spinner("ğŸ§  AIãŒæ¤œè¨¼çµæœã‚’åˆ†æä¸­..."):
                        import time
                        time.sleep(1)  # æ€è€ƒéç¨‹ã®æ¼”å‡º
                
                # AIå›ç­”ç”Ÿæˆã¨è¡¨ç¤º
                response_placeholder = st.empty()
                
                with st.spinner("AIãŒå›ç­”ç”Ÿæˆä¸­..."):
                    response = _generate_qa_response(llm_service, question, search_results)
                
                # å…ƒã®ã‚·ãƒ³ãƒ—ãƒ«ãªinfoè¡¨ç¤º
                with response_placeholder.container():
                    st.info(response)
                
                # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                st.success("âœ… RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸ")
                    
            except Exception as e:
                st.error(f"AIå›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                logger.error(f"QA error: {e}")
        else:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

def render_bedrock_streaming_response(llm_service, question: str, search_results: List[Dict], show_thinking: bool):
    """AWS Bedrockã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’è¡¨ç¤º"""
    progress_bar = st.progress(0)
    spinner_placeholder = st.empty()
    thinking_container = st.empty()
    answer_container = st.empty()
    
    # ç´¯ç©å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
    accumulated_answer = ""
    
    def bedrock_progress_callback(progress: float, message: str):
        nonlocal accumulated_answer
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
        if progress > 1.0:
            progress = 1.0
        elif progress < 0.0:
            progress = 0.0
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
        progress_bar.progress(progress)
        
        # ã‚¹ãƒ”ãƒŠãƒ¼è¡¨ç¤ºï¼ˆå®Ÿè¡Œä¸­... xx%æ¸ˆã¿ï¼‰
        with spinner_placeholder:
            with st.spinner(f"å®Ÿè¡Œä¸­... {progress*100:.1f}%æ¸ˆã¿"):
                # é€”ä¸­æ€è€ƒã‚’ç´¯ç©è¡¨ç¤º
                if isinstance(message, str) and len(message.strip()) > 0:
                    accumulated_answer += message
                    
                    if show_thinking:
                        thinking_container.info(f"ğŸ’­ AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ€è€ƒ: {accumulated_answer}")
                    
                    # å›ç­”ã‚’è¡¨ç¤º
                    answer_container.markdown(f"**å›ç­”:**\n\n{accumulated_answer}")
    
    try:
        # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
        context = _prepare_rag_context(search_results)
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’ç”Ÿæˆ
        if hasattr(llm_service, '_generate_bedrock_streaming'):
            system_prompt = """ã‚ãªãŸã¯é€šä¿¡è¨­å‚™ã®æ¤œè¨¼ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
æ¤œè¨¼ãƒãƒƒãƒã®å®Ÿè¡Œçµæœã«é–¢ã™ã‚‹è³ªå•ã«ã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’åŸºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

å›ç­”ã¯ä»¥ä¸‹ã®å½¢å¼ã§è¡Œã£ã¦ãã ã•ã„ï¼š
1. è³ªå•ã«å¯¾ã™ã‚‹ç›´æ¥çš„ãªå›ç­”
2. é–¢é€£ã™ã‚‹æ¤œè¨¼çµæœã®è©³ç´°
3. æ¨å¥¨äº‹é …ã‚„æ³¨æ„ç‚¹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«ãªã„å†…å®¹ã«ã¤ã„ã¦ã¯ã€ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã§ã¯ç¢ºèªã§ãã¾ã›ã‚“ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚"""
            
            full_prompt = f"""
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:
{context}

è³ªå•: {question}

ä¸Šè¨˜ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’åŸºã«ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
            
            response = llm_service._generate_bedrock_streaming(
                full_prompt, 
                system_prompt, 
                bedrock_progress_callback
            )
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            response = _generate_qa_response(llm_service, question, search_results)
            answer_container.markdown(f"**å›ç­”:**\n\n{response}")
        
        # å®Œäº†è¡¨ç¤º
        progress_bar.progress(1.0)
        spinner_placeholder.empty()
        st.success("âœ… å›ç­”å®Œäº†")
        
    except Exception as e:
        st.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        logger.error(f"Bedrock streaming error: {e}")

def render_ollama_step_response(llm_service, question: str, search_results: List[Dict], show_thinking: bool):
    """Ollamaã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤ºå¿œç­”ã‚’è¡¨ç¤º"""
    progress_bar = st.progress(0)
    spinner_placeholder = st.empty()
    step_container = st.empty()
    answer_container = st.empty()
    
    def ollama_progress_callback(progress: float, message: str):
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
        if progress > 1.0:
            progress = 1.0
        elif progress < 0.0:
            progress = 0.0
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
        progress_bar.progress(progress)
        
        # ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤º
        step_messages = {
            0.0: "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’åˆ†æä¸­...",
            0.3: "é–¢é€£ã™ã‚‹æ¤œè¨¼çµæœã‚’æ•´ç†ä¸­...",
            0.6: "å›ç­”ã‚’ç”Ÿæˆä¸­...",
            0.9: "å›ç­”ã‚’æœ€çµ‚èª¿æ•´ä¸­..."
        }
        
        current_step = "å›ç­”ã‚’ç”Ÿæˆä¸­..."
        for threshold, step_msg in step_messages.items():
            if progress >= threshold:
                current_step = step_msg
        
        # ã‚¹ãƒ”ãƒŠãƒ¼ã§é€²æ—è¡¨ç¤º
        with spinner_placeholder:
            with st.spinner(f"å®Ÿè¡Œä¸­... {progress*100:.1f}%æ¸ˆã¿"):
                if show_thinking:
                    step_container.info(f"ğŸ”„ {current_step}")
    
    try:
        # æ®µéšçš„ã«é€²æ—ã‚’æ›´æ–°ã—ãªãŒã‚‰å›ç­”ç”Ÿæˆ
        ollama_progress_callback(0.1, "é–‹å§‹")
        
        # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
        context = _prepare_rag_context(search_results)
        ollama_progress_callback(0.3, "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™å®Œäº†")
        
        # å›ç­”ç”Ÿæˆ
        response = _generate_qa_response(llm_service, question, search_results)
        ollama_progress_callback(0.9, "å›ç­”ç”Ÿæˆå®Œäº†")
        
        # å›ç­”è¡¨ç¤º
        answer_container.markdown(f"**å›ç­”:**\n\n{response}")
        
        # å®Œäº†è¡¨ç¤º
        progress_bar.progress(1.0)
        spinner_placeholder.empty()
        st.success("âœ… å›ç­”å®Œäº†")
        
    except Exception as e:
        st.error(f"ã‚¹ãƒ†ãƒƒãƒ—å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        logger.error(f"Ollama step error: {e}")

def render_normal_response(llm_service, question: str, search_results: List[Dict]):
    """é€šå¸¸ã®å¿œç­”ã‚’è¡¨ç¤º"""
    with st.spinner("AIå›ç­”ã‚’ç”Ÿæˆä¸­..."):
        try:
            response = _generate_qa_response(llm_service, question, search_results)
            st.markdown(f"**å›ç­”:**\n\n{response}")
            st.success("âœ… å›ç­”å®Œäº†")
        except Exception as e:
            st.error(f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.error(f"Normal response error: {e}")

def _search_batches_directly(question: str, batches: List[Dict], top_k: int = 5) -> List[Dict]:
    """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥é–¢é€£ã™ã‚‹ã‚‚ã®ã‚’æ¤œç´¢"""
    question_lower = question.lower()
    scored_batches = []
    
    for batch in batches:
        score = 0
        
        # åå‰ã§ã®ä¸€è‡´
        if any(word in batch.get('name', '').lower() for word in question_lower.split()):
            score += 3
        
        # è©¦é¨“ãƒ–ãƒ­ãƒƒã‚¯ã§ã®ä¸€è‡´
        if any(word in batch.get('test_block', '').lower() for word in question_lower.split()):
            score += 2
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ã®ä¸€è‡´
        if 'å¤±æ•—' in question_lower and batch.get('status') == 'failed':
            score += 5
        if 'æˆåŠŸ' in question_lower and batch.get('status') == 'completed':
            score += 3
        if 'å®Ÿè¡Œä¸­' in question_lower and batch.get('status') == 'running':
            score += 5
        
        # ä»Šæ—¥ã®æ¤œè¨¼
        if 'ä»Šæ—¥' in question_lower or 'æœ¬æ—¥' in question_lower:
            created_at = batch.get('created_at', '')
            if '2025-09-07' in created_at:  # ä»Šæ—¥ã®æ—¥ä»˜
                score += 4
        
        # è¨­å‚™ã‚¿ã‚¤ãƒ—ã§ã®ä¸€è‡´
        if 'ericsson' in question_lower or 'mmu' in question_lower:
            # çµæœã‹ã‚‰è¨­å‚™ã‚¿ã‚¤ãƒ—ã‚’ç¢ºèª
            results = batch.get('results', [])
            for result in results:
                if 'Ericsson-MMU' in result.get('equipment_type', ''):
                    score += 2
                    break
        
        if score > 0:
            batch_copy = batch.copy()
            batch_copy['search_score'] = score
            scored_batches.append(batch_copy)
    
    # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    scored_batches.sort(key=lambda x: x['search_score'], reverse=True)
    
    return scored_batches[:top_k]

def _prepare_rag_context(search_results: List[Dict]) -> str:
    """RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™"""
    if not search_results:
        return "é–¢é€£ã™ã‚‹æ¤œè¨¼ãƒãƒƒãƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    context_parts = []
    for i, result in enumerate(search_results):
        context_parts.append(f"""
æ¤œè¨¼ãƒãƒƒãƒ {i+1}:
- åå‰: {result.get('name', 'Unknown')}
- å®Ÿè¡Œæ—¥: {result.get('created_at', 'Unknown')}
- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.get('status', 'Unknown')}
- è©¦é¨“ãƒ–ãƒ­ãƒƒã‚¯: {result.get('test_block', 'Unknown')}""")
        
        # çµæœã®è©³ç´°
        if result.get('results'):
            results = result['results']
            success_count = len([r for r in results if r.get('result') == 'PASS'])
            fail_count = len([r for r in results if r.get('result') == 'FAIL'])
            total_count = len(results)
            
            context_parts.append(f"""- æ¤œè¨¼çµæœ: æˆåŠŸ {success_count}ä»¶ã€å¤±æ•— {fail_count}ä»¶ã€åˆè¨ˆ {total_count}ä»¶
- æˆåŠŸç‡: {success_count/total_count*100:.1f}%""")
            
            # å¤±æ•—ã—ãŸé …ç›®ã®è©³ç´°
            failed_items = [r for r in results if r.get('result') == 'FAIL']
            if failed_items:
                context_parts.append("- å¤±æ•—ã—ãŸæ¤œè¨¼é …ç›®:")
                for item in failed_items[:3]:  # æœ€å¤§3ä»¶ã¾ã§
                    condition = item.get('condition_text', item.get('test_condition', 'Unknown'))
                    equipment = item.get('equipment_type', 'Unknown')
                    reason = item.get('failure_reason', item.get('analysis', 'è©³ç´°ä¸æ˜'))
                    context_parts.append(f"  * æ¡ä»¶: {condition}")
                    context_parts.append(f"    è¨­å‚™: {equipment}")
                    context_parts.append(f"    å¤±æ•—ç†ç”±: {reason}")
            
            # æˆåŠŸã—ãŸé …ç›®ã®ä¾‹
            success_items = [r for r in results if r.get('result') == 'PASS']
            if success_items and len(success_items) <= 3:
                context_parts.append("- æˆåŠŸã—ãŸæ¤œè¨¼é …ç›®:")
                for item in success_items[:2]:  # æœ€å¤§2ä»¶ã¾ã§
                    condition = item.get('condition_text', item.get('test_condition', 'Unknown'))
                    equipment = item.get('equipment_type', 'Unknown')
                    context_parts.append(f"  * æ¡ä»¶: {condition} (è¨­å‚™: {equipment})")
        
        # ãƒ†ã‚¹ãƒˆé …ç›®ã®è©³ç´°
        if result.get('test_items'):
            test_items = result['test_items']
            context_parts.append(f"- ãƒ†ã‚¹ãƒˆé …ç›®æ•°: {len(test_items)}ä»¶")
            for item in test_items[:2]:  # æœ€å¤§2ä»¶ã¾ã§
                context_parts.append(f"  * {item.get('condition_text', 'Unknown')}")
    
    return "\n".join(context_parts)

def _generate_qa_response(llm_service, question: str, search_results: List[Dict]) -> str:
    """QAå¿œç­”ã‚’ç”Ÿæˆ"""
    context = _prepare_rag_context(search_results)
    
    system_prompt = """ã‚ãªãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å‚™æ¤œè¨¼ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚RAGã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã£ã¦æ¤œç´¢ã•ã‚ŒãŸé–¢é€£æ¤œè¨¼ãƒãƒƒãƒã‚’åŸºã«ã€è³ªå•ã«å¯¾ã—ã¦æ­£ç¢ºã§å®Ÿç”¨çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

ã€å›ç­”æŒ‡é‡ã€‘
- æ¤œç´¢ã•ã‚ŒãŸæ¤œè¨¼ãƒãƒƒãƒã®å†…å®¹ã‚’æœ€å„ªå…ˆã«æ´»ç”¨ã—ã¦ãã ã•ã„
- æ¤œè¨¼çµæœã«è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„å†…å®¹ã¯æ¨æ¸¬ã›ãšã€ã€Œæ¤œè¨¼çµæœã«è¨˜è¼‰ãªã—ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å‚™ã®å°‚é–€ç”¨èªã‚’æ­£ç¢ºã«ä½¿ç”¨ã—ã¦ãã ã•ã„
- å›ç­”ã¯ç°¡æ½”ã§èª­ã¿ã‚„ã™ã„å½¢å¼ã§æä¾›ã—ã¦ãã ã•ã„

ã€å›ç­”å½¢å¼ã€‘
ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆè¨˜å·ã¯ä½¿ç”¨ã›ãšã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆã§ï¼‰ï¼š

å›ç­”: [è³ªå•ã¸ã®ç›´æ¥çš„ãªå›ç­”]

æ ¹æ‹ : [å›ç­”ã®æ ¹æ‹ ã¨ãªã‚‹å…·ä½“çš„ãªæ¤œè¨¼ãƒãƒƒãƒæƒ…å ±ï¼ˆãƒãƒƒãƒåã¨çµæœã‚’æ˜è¨˜ï¼‰]

è¿½åŠ æƒ…å ±: [æ¤œç´¢ã•ã‚ŒãŸæ¤œè¨¼ãƒãƒƒãƒã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹é–¢é€£ã™ã‚‹é‡è¦ãªè£œè¶³æƒ…å ±]

æ¨å¥¨å¯¾å¿œ: [å¿…è¦ã«å¿œã˜ãŸå…·ä½“çš„ãªæ¨å¥¨å¯¾å¿œç­–]

ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å‚™æ¤œè¨¼ã®å°‚é–€çŸ¥è­˜ã¨RAGã‚·ã‚¹ãƒ†ãƒ ã§æ¤œç´¢ã•ã‚ŒãŸæœ€æ–°æƒ…å ±ã‚’çµ„ã¿åˆã‚ã›ã€å®Ÿç”¨çš„ã§å…·ä½“çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"""
    
    prompt = f"""
è³ªå•: {question}

æ¤œç´¢ã•ã‚ŒãŸé–¢é€£æ¤œè¨¼ãƒãƒƒãƒ:
{context}

ä¸Šè¨˜ã®æ¤œè¨¼ãƒãƒƒãƒæƒ…å ±ã‚’åŸºã«ã€æŒ‡å®šã•ã‚ŒãŸå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
    
    return llm_service.generate_response(prompt, system_prompt)
