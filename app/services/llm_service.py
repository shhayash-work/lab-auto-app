"""
LLMã‚µãƒ¼ãƒ“ã‚¹
LLM Service for validation and analysis

è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œ:
- Ollama (ãƒ­ãƒ¼ã‚«ãƒ«)
- OpenAI GPT-4o
- Anthropic Claude
- AWS Bedrock
"""
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import json
import logging
import os
from typing import Dict, Any, Optional, List
from datetime import datetime
import asyncio

try:
    import ollama
except ImportError:
    ollama = None

try:
    import openai
except ImportError:
    openai = None

try:
    import anthropic
except ImportError:
    anthropic = None

try:
    import boto3
except ImportError:
    boto3 = None

from app.config.settings import (
    OLLAMA_BASE_URL, OLLAMA_MODEL,
    OPENAI_API_KEY, OPENAI_MODEL,
    ANTHROPIC_API_KEY, ANTHROPIC_MODEL,
    AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN, AWS_REGION, BEDROCK_MODEL
)
# knowledge_serviceã¯é…å»¶ãƒ­ãƒ¼ãƒ‰ã§ä½¿ç”¨

logger = logging.getLogger(__name__)

class LLMService:
    """LLMã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, provider: str = "ollama"):
        self.provider = provider
        self.client = None
        
        # knowledge_serviceï¼ˆé…å»¶ãƒ­ãƒ¼ãƒ‰ï¼‰
        self._knowledge_service = None
        self._setup_client()
    
    def _get_knowledge_service(self):
        """knowledge_serviceã‚’é…å»¶ãƒ­ãƒ¼ãƒ‰"""
        if self._knowledge_service is None:
            try:
                from app.services.knowledge_service import get_knowledge_service
                self._knowledge_service = get_knowledge_service()
            except Exception as e:
                logger.warning(f"Knowledge service initialization failed: {e}")
                self._knowledge_service = None
        return self._knowledge_service
    
    def _setup_client(self):
        """ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            if self.provider == "ollama":
                self._setup_ollama()
            elif self.provider == "openai":
                self._setup_openai()
            elif self.provider == "anthropic":
                self._setup_anthropic()
            elif self.provider == "bedrock":
                self._setup_bedrock()
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
        except Exception as e:
            logger.error(f"Failed to setup {self.provider}: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Ollamaã‚’è©¦ã™
            if self.provider != "ollama":
                logger.info("Falling back to Ollama")
                self.provider = "ollama"
                self._setup_ollama()
    
    def _setup_ollama(self):
        """Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if ollama is None:
            raise ImportError("ollama package not installed")
        
        self.client = ollama.Client(host=OLLAMA_BASE_URL)
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        try:
            models = self.client.list()
            if hasattr(models, 'models'):
                # Pydanticãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                available_models = []
                for model in models.models:
                    if hasattr(model, 'name'):
                        available_models.append(model.name)
                    elif hasattr(model, 'model'):
                        available_models.append(model.model)
                    elif isinstance(model, dict):
                        available_models.append(model.get('name', model.get('model', '')))
            elif isinstance(models, dict):
                available_models = [model.get('name', '') for model in models.get('models', [])]
            else:
                available_models = []
            
            if OLLAMA_MODEL not in available_models:
                logger.warning(f"Model {OLLAMA_MODEL} not found. Available: {available_models}")
            logger.info(f"âœ… Ollama connected: {OLLAMA_BASE_URL}")
        except Exception as e:
            logger.error(f"âŒ Ollama connection failed: {e}")
            raise
    
    def _setup_openai(self):
        """OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY not set")
        
        if openai is None:
            raise ImportError("openai package not installed")
        
        self.client = openai.OpenAI(api_key=OPENAI_API_KEY)
        logger.info("âœ… OpenAI client initialized")
    
    def _setup_anthropic(self):
        """Anthropicã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if not ANTHROPIC_API_KEY:
            raise ValueError("ANTHROPIC_API_KEY not set")
        
        if anthropic is None:
            raise ImportError("anthropic package not installed")
        
        self.client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        logger.info("âœ… Anthropic client initialized")
    
    def _setup_bedrock(self):
        """AWS Bedrockã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if not AWS_ACCESS_KEY_ID or not AWS_SECRET_ACCESS_KEY:
            raise ValueError("AWS credentials not set")
        
        if boto3 is None:
            raise ImportError("boto3 package not installed")
        
        # AWS_SESSION_TOKENãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸€æ™‚çš„ãªèªè¨¼æƒ…å ±ã‚’ä½¿ç”¨
        client_kwargs = {
            'service_name': 'bedrock-runtime',
            'aws_access_key_id': AWS_ACCESS_KEY_ID,
            'aws_secret_access_key': AWS_SECRET_ACCESS_KEY,
            'region_name': AWS_REGION
        }
        
        # AWS_SESSION_TOKENãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆAssumeRoleä½¿ç”¨æ™‚ï¼‰
        if AWS_SESSION_TOKEN:
            client_kwargs['aws_session_token'] = AWS_SESSION_TOKEN
            logger.info("ğŸ” Using temporary credentials with session token")
        
        self.client = boto3.client(**client_kwargs)
        logger.info("âœ… AWS Bedrock client initialized")
    
    def generate_response(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
        try:
            if self.provider == "ollama":
                return self._generate_ollama(prompt, system_prompt)
            elif self.provider == "openai":
                return self._generate_openai(prompt, system_prompt)
            elif self.provider == "anthropic":
                return self._generate_anthropic(prompt, system_prompt)
            elif self.provider == "bedrock":
                return self._generate_bedrock(prompt, system_prompt)
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            return f"ã‚¨ãƒ©ãƒ¼: LLMå¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ ({str(e)})"
    
    def _generate_ollama(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """Ollamaå¿œç­”ã‚’ç”Ÿæˆ"""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = self.client.chat(
            model=OLLAMA_MODEL,
            messages=messages,
            options={
                "temperature": 0.7,
                "top_p": 0.9,
                "num_predict": 2048,  # å…ƒã«æˆ»ã™
                "num_ctx": 10240     # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ‹¡å¼µ
            }
        )
        return response['message']['content']
    
    def _generate_openai(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """OpenAIå¿œç­”ã‚’ç”Ÿæˆ"""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = self.client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=messages,
            temperature=0.7,
            max_tokens=2048
        )
        return response.choices[0].message.content
    
    def _generate_anthropic(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """Anthropicå¿œç­”ã‚’ç”Ÿæˆ"""
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"
        
        response = self.client.messages.create(
            model=ANTHROPIC_MODEL,
            max_tokens=2048,
            temperature=0.7,
            messages=[{"role": "user", "content": full_prompt}]
        )
        return response.content[0].text
    
    def _generate_bedrock(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """AWS Bedrockå¿œç­”ã‚’ç”Ÿæˆ"""
        try:
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"
            
            body = json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2048,
                "temperature": 0.7,
                "messages": [{"role": "user", "content": full_prompt}]
            })
            
            logger.info(f"Bedrock request body: {body[:200]}...")
            
            response = self.client.invoke_model(
                modelId=BEDROCK_MODEL,
                body=body
            )
            
            response_body = json.loads(response['body'].read())
            logger.info(f"Bedrock response: {response_body}")
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’ç¢ºèª
            if 'content' in response_body and response_body['content']:
                return response_body['content'][0]['text']
            elif 'completion' in response_body:
                return response_body['completion']
            else:
                logger.error(f"Unexpected Bedrock response structure: {response_body}")
                raise ValueError(f"Unexpected response structure: {response_body}")
                
        except Exception as e:
            logger.error(f"Bedrock generation error: {e}")
            raise
    
    def _generate_bedrock_streaming(self, prompt: str, system_prompt: Optional[str] = None, progress_callback=None) -> str:
        """AWS Bedrockå¿œç­”ã‚’ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰"""
        try:
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"
            
            body = json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 2048,
                "temperature": 0.7,
                "messages": [{"role": "user", "content": full_prompt}]
            })
            
            logger.info(f"Bedrock streaming request body: {body[:200]}...")
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½¿ç”¨
            response = self.client.invoke_model_with_response_stream(
                modelId=BEDROCK_MODEL,
                body=body
            )
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
            full_response = ""
            chunk_count = 0
            total_chunks_estimate = 20  # æ¨å®šãƒãƒ£ãƒ³ã‚¯æ•°
            
            for event in response['body']:
                if 'chunk' in event:
                    chunk = json.loads(event['chunk']['bytes'].decode())
                    
                    if chunk['type'] == 'content_block_delta':
                        if 'delta' in chunk and 'text' in chunk['delta']:
                            text_chunk = chunk['delta']['text']
                            full_response += text_chunk
                            chunk_count += 1
                            
                            # é€²æ—ã¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚­ã‚¹ãƒˆã‚’å ±å‘Š
                            progress = min(chunk_count / total_chunks_estimate, 0.95)
                            if progress_callback:
                                progress_callback(progress, text_chunk)
                    
                    elif chunk['type'] == 'message_stop':
                        # å®Œäº†
                        if progress_callback:
                            progress_callback(1.0, "å¿œç­”å®Œäº†")
                        break
            
            logger.info(f"Bedrock streaming response completed: {len(full_response)} chars")
            return full_response
            
        except Exception as e:
            logger.error(f"Bedrock streaming generation error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®APIå‘¼ã³å‡ºã—
            logger.info("Falling back to non-streaming Bedrock API")
            return self._generate_bedrock(prompt, system_prompt)
    
    def analyze_validation_result(self, test_item: Dict[str, Any], equipment_response: Dict[str, Any]) -> Dict[str, Any]:
        """æ¤œè¨¼çµæœã‚’åˆ†æ"""
        system_prompt = """ã‚ãªãŸã¯é€šä¿¡è¨­å‚™ã®æ¤œè¨¼ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
åŸºåœ°å±€è¨­å‚™ã‹ã‚‰ã®å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ãƒ†ã‚¹ãƒˆé …ç›®ã®åˆ¤å®šã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

åˆ¤å®šåŸºæº–:
- PASS: æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã€ã™ã¹ã¦ã®æ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹
- FAIL: æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œãŒå®Ÿè¡Œã•ã‚Œãªã„ã€ã¾ãŸã¯æ˜ç¢ºã«æ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ãªã„
- NEEDS_CHECK: çµæœãŒæ›–æ˜§ã€äºˆæœŸã—ãªã„å€¤ã€ã¾ãŸã¯åˆ¤æ–­ã«è¿·ã†å ´åˆ

å¿œç­”ã¯å¿…ãšJSONå½¢å¼ã§ä»¥ä¸‹ã®æ§‹é€ ã«ã—ã¦ãã ã•ã„:
{
    "result": "PASS|FAIL|NEEDS_CHECK",
    "confidence": 0.0-1.0,
    "analysis": "è©³ç´°ãªåˆ†æå†…å®¹",
    "issues": ["å•é¡Œç‚¹ã®ãƒªã‚¹ãƒˆ"],
    "recommendations": ["æ¨å¥¨äº‹é …ã®ãƒªã‚¹ãƒˆ"]
}"""
        
        prompt = f"""
ãƒ†ã‚¹ãƒˆé …ç›®:
- ã‚«ãƒ†ã‚´ãƒª: {test_item.get('category', 'N/A')}
- æ¡ä»¶: {test_item.get('condition', {}).get('condition_text', 'N/A')}

è¨­å‚™å¿œç­”ãƒ‡ãƒ¼ã‚¿:
{json.dumps(equipment_response, ensure_ascii=False, indent=2)}

ã“ã®å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ãƒ†ã‚¹ãƒˆé …ç›®ã®åˆæ ¼/ä¸åˆæ ¼ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            response = self.generate_response(prompt, system_prompt)
            
            # JSONãƒ‘ãƒ¼ã‚¹ã‚’è©¦è¡Œï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡ºï¼‰
            try:
                # ã¾ãšç›´æ¥JSONãƒ‘ãƒ¼ã‚¹ã‚’è©¦è¡Œ
                result = json.loads(response)
            except json.JSONDecodeError:
                # å¤±æ•—ã—ãŸå ´åˆã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡º
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                
                if json_start != -1 and json_end > json_start:
                    json_str = response[json_start:json_end]
                    try:
                        result = json.loads(json_str)
                    except json.JSONDecodeError as e:
                        logger.error(f"Failed to parse extracted JSON: {e}")
                        logger.error(f"Extracted JSON: {json_str}")
                        raise ValueError(f"LLM returned invalid JSON: {str(e)}")
                else:
                    logger.error(f"No JSON found in response: {response}")
                    raise ValueError("No JSON found in LLM response")
            
            # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¤œè¨¼
            required_fields = ['result', 'confidence', 'analysis']
            for field in required_fields:
                if field not in result:
                    result[field] = self._get_default_value(field)
            
            # å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
            if not isinstance(result['confidence'], (int, float)) or not 0 <= result['confidence'] <= 1:
                result['confidence'] = 0.8
            
            if result['result'] not in ['PASS', 'FAIL', 'WARNING']:
                result['result'] = 'FAIL'
            
            return result
                
        except Exception as e:
            logger.error(f"LLM analysis failed: {e}")
            raise
    
    def _get_default_value(self, field: str) -> Any:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å–å¾—"""
        defaults = {
            'result': 'FAIL',
            'confidence': 0.5,
            'analysis': 'LLMåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ',
            'issues': [],
            'recommendations': []
        }
        return defaults.get(field, None)
    
    
    def generate_test_items(self, feature_name: str, equipment_types: List[str], progress_callback=None) -> List[Dict[str, Any]]:
        """æ¤œè¨¼é …ç›®ã‚’ç”Ÿæˆï¼ˆçŸ¥è¦‹å­¦ç¿’æ©Ÿèƒ½ä»˜ãï¼‰"""
        
        # çŸ¥è¦‹å­¦ç¿’ã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–
        base_system_prompt = """ã‚ãªãŸã¯é€šä¿¡è¨­å‚™ã®æ¤œè¨¼ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
æ–°æ©Ÿèƒ½ã«å¯¾ã™ã‚‹æ¤œè¨¼é …ç›®ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®è¦³ç‚¹ã‚’å«ã‚ã¦ãã ã•ã„:
1. CMãƒ‡ãƒ¼ã‚¿ã®å–å¾—
2. å„ç¨®ãƒ•ã‚£ãƒ«ã‚¿å‡¦ç†
3. æ­£å¸¸ç³»ãƒ»æº–æ­£å¸¸ç³»ãƒ»ç•°å¸¸ç³»ã®ã‚·ãƒŠãƒªã‚ª
4. è¨­å‚™ã‚¿ã‚¤ãƒ—åˆ¥ã®æ¤œè¨¼

å¿œç­”ã¯å¿…ãšJSONé…åˆ—å½¢å¼ã§ã€ä»¥ä¸‹ã®æ§‹é€ ã«ã—ã¦ãã ã•ã„:
[
    {
        "test_block": "è©¦é¨“ãƒ–ãƒ­ãƒƒã‚¯å",
        "category": "æ¤œè¨¼ã‚«ãƒ†ã‚´ãƒª",
        "condition_text": "æ¤œè¨¼æ¡ä»¶ã®è©³ç´°"
    }
]

æ¤œè¨¼æ¡ä»¶ã¯å¯¾è±¡è¨­å‚™ã§ã®æˆåŠŸãƒ»å¤±æ•—ã‚’åˆ¤å®šã™ã‚‹ãŸã‚ã®å…·ä½“çš„ãªæ¡ä»¶ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚"""
        
        # çŸ¥è¦‹å­¦ç¿’æ©Ÿèƒ½ã«ã‚ˆã‚‹å¼·åŒ–
        enhanced_system_prompt = base_system_prompt
        knowledge_service = self._get_knowledge_service()
        if knowledge_service:
            try:
                enhanced_system_prompt = knowledge_service.enhance_item_generation_prompt(
                    base_system_prompt, feature_name, equipment_types
                )
            except Exception as e:
                logger.warning(f"Knowledge enhancement failed: {e}")
        
        system_prompt = enhanced_system_prompt
        
        # RAGãƒ™ã‚¯ã‚¿ãƒ¼DBã‹ã‚‰é–¢é€£ã™ã‚‹éå»ã®æ¤œè¨¼é …ç›®ã‚’æ¤œç´¢
        if progress_callback:
            progress_callback(0.5, "RAGãƒ™ã‚¯ã‚¿ãƒ¼DBã‹ã‚‰é¡ä¼¼æ¤œè¨¼é …ç›®ã‚’æ¤œç´¢ä¸­...")
        rag_context = self._search_similar_test_items(feature_name, equipment_types)
        
        prompt = f"""
æ©Ÿèƒ½å: {feature_name}
å¯¾è±¡è¨­å‚™: {', '.join(equipment_types)}

ã€éå»ã®é¡ä¼¼æ¤œè¨¼é …ç›®ï¼ˆRAGæ¤œç´¢çµæœï¼‰ã€‘
{rag_context}

ä¸Šè¨˜ã®éå»ã®æ¤œè¨¼é …ç›®ã‚’å‚è€ƒã«ã€æ–°ã—ã„æ©Ÿèƒ½ã€Œ{feature_name}ã€ã«ã¤ã„ã¦åŒ…æ‹¬çš„ãªæ¤œè¨¼é …ç›®ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
åŸºåœ°å±€ã‚¹ãƒªãƒ¼ãƒ—æ©Ÿèƒ½ã®æ¤œè¨¼ã‚’å‚è€ƒã«ã€ä»¥ä¸‹ã®è¦³ç‚¹ã‚’å«ã‚ã¦ãã ã•ã„:

1. ESGé¸å®š
2. CMãƒ‡ãƒ¼ã‚¿ã®å–å¾—
3. ã‚¤ãƒ³ãƒ‰ã‚¢å¯¾ç­–å±€ã®ãƒ•ã‚£ãƒ«ã‚¿
4. å¯¾ç­–ãƒãƒ³ãƒ‰ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿
5. ESGä½œæˆ
6. ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆå±€ã®ãƒ•ã‚£ãƒ«ã‚¿
7. ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆå±€ã®ãƒ•ã‚£ãƒ«ã‚¿
8. ä½œæ¥­ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿

å„è¨­å‚™ã‚¿ã‚¤ãƒ—ï¼ˆ{', '.join(equipment_types)}ï¼‰ã«å¯¾å¿œã—ãŸæ¤œè¨¼é …ç›®ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            if progress_callback:
                progress_callback(0.7, f"{self.provider.upper()} AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ¤œè¨¼é …ç›®ã‚’ç”Ÿæˆä¸­...")
            
            # Bedrockã®å ´åˆã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ
            if self.provider == "bedrock":
                response = self._generate_bedrock_streaming(prompt, system_prompt, progress_callback)
            else:
                response = self.generate_response(prompt, system_prompt)
                if progress_callback:
                    progress_callback(0.9, "ç”Ÿæˆã•ã‚ŒãŸæ¤œè¨¼é …ç›®ã‚’è§£æä¸­...")
            
            try:
                test_items = json.loads(response)
                if isinstance(test_items, list):
                    logger.info(f"Generated {len(test_items)} test items using RAG")
                    
                    if progress_callback:
                        progress_callback(1.0, f"æ¤œè¨¼é …ç›®ç”Ÿæˆå®Œäº†: {len(test_items)}ä»¶")
                    
                    return test_items
                else:
                    logger.error("LLM response is not a list")
                    raise ValueError("LLM returned invalid format (not a list)")
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse LLM response as JSON: {e}")
                raise ValueError(f"LLM returned invalid JSON: {str(e)}")
                
        except Exception as e:
            logger.error(f"Test item generation failed: {e}")
            if progress_callback:
                progress_callback(1.0, f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def _search_similar_test_items(self, feature_name: str, equipment_types: List[str]) -> str:
        """RAGãƒ™ã‚¯ã‚¿ãƒ¼DBã‹ã‚‰é¡ä¼¼ã™ã‚‹æ¤œè¨¼é …ç›®ã‚’æ¤œç´¢"""
        try:
            from app.services.vector_store import get_vector_store
            
            # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ä½œæˆ
            query = f"{feature_name} {' '.join(equipment_types)} æ¤œè¨¼é …ç›®"
            
            # ãƒ™ã‚¯ã‚¿ãƒ¼DBã‹ã‚‰é¡ä¼¼é …ç›®ã‚’æ¤œç´¢
            vector_store = get_vector_store()
            similar_items = vector_store.search_similar_documents(query, top_k=5)
            
            if similar_items:
                context = "éå»ã®é¡ä¼¼æ¤œè¨¼é …ç›®:\n"
                for i, item in enumerate(similar_items, 1):
                    context += f"{i}. {item.get('content', '')}\n"
                logger.info(f"Found {len(similar_items)} similar test items from RAG")
                return context
            else:
                logger.info("No similar test items found in RAG")
                return "éå»ã®é¡ä¼¼æ¤œè¨¼é …ç›®ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                
        except Exception as e:
            logger.warning(f"RAG search failed: {e}")
            return "RAGæ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚åŸºæœ¬çš„ãªæ¤œè¨¼é …ç›®ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
    

# ã‚°ãƒ­ãƒ¼ãƒãƒ«LLMã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
def get_llm_service(provider: str = "ollama") -> LLMService:
    """LLMã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    return LLMService(provider=provider)
